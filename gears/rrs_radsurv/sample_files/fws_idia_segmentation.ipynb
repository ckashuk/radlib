{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9O2FMEC9OeU"
   },
   "source": [
    "# Overview\n",
    "**BRAIN SEGMENTATION INFERENCE**\n",
    "This Jupyter notebook is designed to run inference on a brain MRI scan using a pre-trained segmentation model. It downloads a model from the cloud (requiring AWS access keys to be set up properly) and then runs inference on a single scan. The output is a nifti file with the segmentation labels. You may wish to then load it in a program such as Slicer to view the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjoOMwOTNJTd",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Setup\n",
    "First, set up a python environment with the necessary packages.\n",
    "\n",
    "For instance, in VSCode or Cursor, press Ctrl+Shift+P and type \"Python: Create Environment\" and follow the prompts.\n",
    "\n",
    "Or, on the command line, run:\n",
    "```bash\n",
    "python -m venv brain_segmentation_inference_env\n",
    "```\n",
    "--or--\n",
    "```bash\n",
    "python3 -m venv brain_segmentation_inference_env\n",
    "```\n",
    "and then either \n",
    "```bash\n",
    "brain_segmentation_inference_env\\Scripts\\activate\n",
    "```\n",
    "on Windows, or\n",
    "```bash\n",
    "source brain_segmentation_inference_env/bin/activate\n",
    "```\n",
    "on macOS and Linux.\n",
    "\n",
    "Then, install the necessary packages:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "After all that, this cell should run without error and import all the necessary packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==2.5.1\n",
      "  Using cached torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from torch==2.5.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\cxk023\\appdata\\roaming\\python\\python311\\site-packages (from torch==2.5.1) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch==2.5.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch==2.5.1) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda\\lib\\site-packages (from torch==2.5.1) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\cxk023\\appdata\\roaming\\python\\python311\\site-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\lib\\site-packages (from jinja2->torch==2.5.1) (2.1.3)\n",
      "Using cached torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "Successfully installed torch-2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe, torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\CXK023\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.15.1 requires torch==2.0.0, but you have torch 2.5.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# pip install -r z:/Glioma_Radsurv/requeriments.txt\n",
    "# !pip install --upgrade typing-extensions\n",
    "!pip install torch==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n",
      "cuda devices (count) .... 0\n"
     ]
    }
   ],
   "source": [
    "#Check GPU availability\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print('cuda devices (count) ....',torch.cuda.device_count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create a named logger\n",
    "logger = logging.getLogger('brain_segmentation')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create a console handler and set its level\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create a formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Add the formatter to the console handler\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the console handler to the logger\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Turn off logs from all other loggers\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name != 'brain_segmentation':\n",
    "        logging.getLogger(name).setLevel(logging.CRITICAL)\n",
    "\n",
    "logger.info('ğŸš€ Setting up logging...')\n",
    "logger.debug(f'ğŸ”§ Current logging level: {logger.getEffectiveLevel()}')\n",
    "logger.info('âœ… Logging setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SdAAZeqmPNsv",
    "outputId": "2706f96f-1b8d-4f0d-9f3e-b0f3c8dbaa2f"
   },
   "outputs": [],
   "source": [
    "logger.info('ğŸ“¦ Importing outside packages...')\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import json, yaml\n",
    "import tempfile\n",
    "import boto3\n",
    "import botocore\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from smart_open import open\n",
    "from monai.transforms import AsDiscrete, Activations\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "logger.info('âœ… Outside packages imported.')\n",
    "\n",
    "logger.info('ğŸ“¦ Importing local packages...')\n",
    "from core_common import get_loader_val, datafold_read\n",
    "import model\n",
    "logger.info('âœ… Local packages imported.')\n",
    "\n",
    "logger.info('ğŸ“¦ Setting up device...')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info('âœ… Device set up.')\n",
    "logger.debug(f'ğŸ–¥ï¸ {device = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OeN_BW_NJTj"
   },
   "source": [
    "# Model\n",
    "This part loads the model from the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX5dkAr0NJTl"
   },
   "source": [
    "## Download\n",
    "This part downloads the model from the cloud.\n",
    "\n",
    "You'll need to have an AWS profile named 'theta-model-downloader'. You can create this profile by running this command in your terminal:\n",
    "```bash\n",
    "aws configure --profile theta-model-downloader\n",
    "```\n",
    "and entering your AWS credentials.\n",
    "\n",
    "If you do not have the AWS command line tools installed, you can install them by following the instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting the input data dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path=\"/app/Data/_Brain/Radiology/_Adult/_Glioma/TCGA-GBM\"\n",
    "\n",
    "cases_list=glob.glob(os.path.join(main_path,\"preprocessed\",\"*\",\"Baseline\",\"*T1c.nii.gz\"))\n",
    "cases_list=sorted(cases_list)\n",
    "\n",
    "new_ids=[]\n",
    "for paths_files in cases_list:\n",
    "    case_id=Path(paths_files)\n",
    "    case_id=case_id.parts[-3]\n",
    "    dir_name=os.path.dirname(paths_files)\n",
    "    files = [dir_name+'/'+case_id+'_FLAIR_reg_SkullS_BiasC.nii.gz', \n",
    "             dir_name+'/'+case_id + '_T1c_SRI24_SkullS_BiasC.nii.gz',\n",
    "             dir_name+'/'+case_id + '_T1_reg_SkullS_BiasC.nii.gz', \n",
    "             dir_name+'/'+case_id + '_T2_reg_SkullS_BiasC.nii.gz']\n",
    "    \n",
    "    all_exist = all([os.path.exists(file) for file in files])\n",
    "    if all_exist:\n",
    "        new_ids.append(case_id)\n",
    "\n",
    "len(new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'training': []\n",
    "}\n",
    "\n",
    "for id_num in new_ids:\n",
    "    new_entry = {\n",
    "        'fold': 1,\n",
    "        'label': f'{id_num}/Baseline/{id_num}_tumor_seg_swinUNETR.nii.gz',\n",
    "        'image': [\n",
    "            f'{id_num}/Baseline/{id_num}_FLAIR_reg_SkullS_BiasC.nii.gz',\n",
    "            f'{id_num}/Baseline/{id_num}_T1c_SRI24_SkullS_BiasC.nii.gz',\n",
    "            f'{id_num}/Baseline/{id_num}_T1_reg_SkullS_BiasC.nii.gz',\n",
    "            f'{id_num}/Baseline/{id_num}_T2_reg_SkullS_BiasC.nii.gz'\n",
    "        ]\n",
    "    }\n",
    "    # Append the new entry to the 'training' list\n",
    "    data['training'].append(new_entry)\n",
    "    \n",
    "with open('./data/multi_example2.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path=\"./data/multi_example2.json\"\n",
    "with open(files_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# check first instance JSON data\n",
    "data[\"training\"][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('ğŸ“‚ Loading configuration file...')\n",
    "config_pth = 'config/example_config.yaml'\n",
    "\n",
    "logger.info('ğŸ”“ Opening configuration file...')\n",
    "with open(config_pth, 'r') as file:\n",
    "    logger.info('ğŸ“– Reading YAML content...')\n",
    "    inference_cfg = yaml.safe_load(file)\n",
    "logger.debug(f'ğŸ”§ {inference_cfg = }')\n",
    "logger.info('âœ… Configuration loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the terminal\n",
    "#!aws configure --profile theta-model-downloader\n",
    "# enter your credential: (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xb1qCskWNJTm",
    "outputId": "a8a36564-66cf-4cf1-c8e9-b7201c6f6b93"
   },
   "outputs": [],
   "source": [
    "# logger.info('ğŸ“‚ Loading configuration file...')\n",
    "# config_pth = 'config/config.yaml'\n",
    "\n",
    "# logger.info('ğŸ”“ Opening configuration file...')\n",
    "# with open(config_pth, 'r') as file:\n",
    "#     logger.info('ğŸ“– Reading YAML content...')\n",
    "#     inference_cfg = yaml.safe_load(file)\n",
    "# logger.debug(f'ğŸ”§ {inference_cfg = }')\n",
    "# logger.info('âœ… Configuration loaded.')\n",
    "\n",
    "logger.info('ğŸ”§ Defining utility function...')\n",
    "def check_optional_key(x: dict, key_name, true_val):\n",
    "    logger.debug(f'ğŸ” Checking for key: {key_name} with value: {true_val}')\n",
    "    result = (key_name in x.keys()) and (x[key_name] == true_val)\n",
    "    logger.debug(f'ğŸ” Result of check: {result}')\n",
    "    return result\n",
    "logger.info('âœ… Utility function defined.')\n",
    "\n",
    "logger.info('â˜ï¸ Setting up AWS S3 connection...')\n",
    "bucket = inference_cfg['model']['bucket']\n",
    "key = inference_cfg['model']['key']\n",
    "logger.info('ğŸ” Creating AWS session...')\n",
    "session = boto3.Session(\n",
    "    profile_name='theta-model-downloader',\n",
    "    region_name=inference_cfg['model']['region'])\n",
    "logger.info('ğŸ”— Creating S3 client...')\n",
    "s3_client = session.client('s3',verify=False)\n",
    "logger.debug(f'ğŸª£ {bucket = }')\n",
    "logger.debug(f'ğŸ”‘ {key = }')\n",
    "logger.info('âœ… AWS S3 connection set up.')\n",
    "\n",
    "logger.info('ğŸ“¦ Fetching model metadata...')\n",
    "logger.info('ğŸ” Retrieving object metadata from S3...')\n",
    "metadata = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "file_size = metadata['ContentLength']\n",
    "logger.info('ğŸ”§ Parsing model configuration from metadata...')\n",
    "model_cfg = yaml.safe_load(metadata['Metadata']['cfg'])\n",
    "logger.debug(f'ğŸ“ {file_size = }')\n",
    "logger.debug(f'ğŸ”§ {model_cfg = }')\n",
    "logger.info('âœ… Model metadata fetched.')\n",
    "\n",
    "logger.info('â¬‡ï¸ Downloading model checkpoint...')\n",
    "logger.info('ğŸ“ Creating temporary file...')\n",
    "with tempfile.NamedTemporaryFile(mode='wb', suffix='.ckpt', delete=False) as temp_file:\n",
    "    logger.info('ğŸ”„ Setting up progress bar...')\n",
    "    with tqdm(total=file_size, unit='B', unit_scale=True, desc='Downloading checkpoint...') as progress_bar:\n",
    "        def update_progress(chunk):\n",
    "            progress_bar.update(chunk)\n",
    "        logger.info('ğŸ”„ Starting file download...')\n",
    "        s3_client.download_fileobj(Bucket=bucket, Key=key, Fileobj=temp_file, Callback=update_progress)\n",
    "\n",
    "    logger.info('ğŸ’¾ Flushing temporary file...')\n",
    "    temp_file.flush()\n",
    "    logger.info('ğŸ”„ Loading checkpoint into memory...')\n",
    "    checkpoint = torch.load(temp_file.name, map_location=device)\n",
    "    logger.info('ğŸ—‘ï¸ Closing and removing temporary file...')\n",
    "    temp_file.close()\n",
    "    os.unlink(temp_file.name) \n",
    "logger.info('âœ… Model checkpoint downloaded and loaded.')\n",
    "\n",
    "logger.info('ğŸ”„ Processing state dictionary...')\n",
    "state_dict = checkpoint['state_dict']\n",
    "logger.info('ğŸ”„ Removing \"model.\" prefix from state dict keys...')\n",
    "state_dict = {key.replace('model.', ''): value for key, value in state_dict.items()}\n",
    "logger.debug(f'ğŸ”‘ State dict keys: {state_dict.keys()}')\n",
    "logger.info('âœ… State dictionary processed.')\n",
    "\n",
    "logger.info('ğŸ”„ Printing model configuration...')\n",
    "pprint.pprint(model_cfg)\n",
    "logger.info('âœ… Model configuration printed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9E4uJj9NJTn"
   },
   "source": [
    "## Load\n",
    "Once the model has been downloaded, this section loads the model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ujk2UMcINJTn",
    "outputId": "e29fca60-dc4b-4df8-b32d-d9f147c95a75"
   },
   "outputs": [],
   "source": [
    "def listify_3d(x: dict):\n",
    "    logger.info('ğŸ§Š Listifying 3D dimensions...')\n",
    "    dimensions = [x['h'], x['w'], x['d']]\n",
    "    logger.debug(f'ğŸ“ Listified dimensions: {dimensions = }')\n",
    "    return dimensions\n",
    "\n",
    "logger.info('ğŸ›ï¸ Fetching hyperparameters...')\n",
    "hparams = model_cfg['hyperparameter']\n",
    "logger.debug(f'ğŸ›ï¸ Hyperparameters: {hparams = }')\n",
    "\n",
    "logger.info('ğŸ”— Fetching label union...')\n",
    "union = model_cfg['data']['label_union']\n",
    "logger.debug(f'ğŸ”— Label union: {union = }')\n",
    "\n",
    "logger.info('ğŸ“ Calculating ROI size...')\n",
    "roi_size = listify_3d(hparams['roi'])\n",
    "logger.debug(f'ğŸ“ ROI size: {roi_size = }')\n",
    "\n",
    "logger.info('ğŸ—ï¸ Creating model instance...')\n",
    "model = model.CustomSwinUNETR(\n",
    "    in_channels       = 4, # one per MRI modality: T1, T2, T1-Contrast, FLAIR\n",
    "    img_size          = roi_size,\n",
    "    out_channels      = 4 if union else 3, # one per label: tumor core, whole tumor, enhancing tumor\n",
    "    feature_size      = hparams['feature_size'],\n",
    "    use_checkpoint    = True,\n",
    "    depths            = hparams['depths'],\n",
    "    num_heads         = hparams['num_heads'],\n",
    "    norm_name         = hparams['norm_name'],\n",
    "    normalize         = hparams['normalize'],\n",
    "    downsample        = hparams['downsample'],\n",
    "    use_v2            = hparams['use_v2'],\n",
    "    mlp_ratio         = hparams['mlp_ratio'],\n",
    "    qkv_bias          = hparams['qkv_bias'],\n",
    "    patch_size        = hparams['patch_size'],\n",
    "    window_size       = hparams['window_size'],\n",
    ")\n",
    "logger.info('âœ… Model instance created.')\n",
    "\n",
    "logger.info('ğŸ”‘ Fetching first model state key...')\n",
    "first_model_state_key = next(iter(model.state_dict().keys()))\n",
    "logger.debug(f'âœ… First model state key: {first_model_state_key = }')\n",
    "\n",
    "logger.info('ğŸ”‘ Fetching first state dict key...')\n",
    "first_state_dict_key = next(iter(state_dict.keys()))\n",
    "logger.debug(f'âœ… First state dict key: {first_state_dict_key = }')\n",
    "\n",
    "logger.info('ğŸ’¾ Loading state dictionary into model...')\n",
    "model.load_state_dict(state_dict)\n",
    "logger.info('âœ… State dictionary loaded.')\n",
    "\n",
    "logger.info('ğŸ–¥ï¸ Moving model to device...')\n",
    "model.to(device)\n",
    "logger.info('âœ… Model moved to device.')\n",
    "\n",
    "logger.info('ğŸ§  Setting model to evaluation mode...')\n",
    "model.eval()\n",
    "logger.info('âœ… Model set to evaluation mode.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYlZMp0kNJTn"
   },
   "source": [
    "# Data\n",
    "This section loads the data defined in your `config/config.yaml` file. Please see the example config file for more details. It requires you to have a nifti file for each MRI modality, typically T1, T2, T1-Contrast, and FLAIR, and the location to store the output nifti segmentation file. There are also instructions for loading multiple scans from a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFiR4fUuNJTo",
    "outputId": "d75fb267-4a8b-4974-91b8-6fc54694707d"
   },
   "outputs": [],
   "source": [
    "logger.info('ğŸ”¢ Setting up fold and configuration...')\n",
    "fold = 1  # Let the validation fold be 1 - same convention as during training.\n",
    "logger.debug(f'ğŸ“Š {fold = }')\n",
    "\n",
    "logger.info('ğŸ” Checking optional configuration keys...')\n",
    "do_ground_truth: bool = check_optional_key(inference_cfg['input'], 'compute_dice', True)\n",
    "logger.debug(f'ğŸ¯ {do_ground_truth = }')\n",
    "use_scan_list: bool = check_optional_key(inference_cfg['input'], 'mode', 'multi')\n",
    "logger.debug(f'ğŸ“‹ {use_scan_list = }')\n",
    "\n",
    "data_dir = inference_cfg['input']['data_dir']\n",
    "logger.debug(f'ğŸ“‚ {data_dir = }')\n",
    "\n",
    "logger.info('ğŸ“ Loading scan data...')\n",
    "if use_scan_list:\n",
    "    logger.info('ğŸ“š Using multiple scans from JSON...')\n",
    "    json_path = inference_cfg['input']['multi_scan']['scan_list']\n",
    "    _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
    "    with open(json_path) as f:\n",
    "        test_instance = json.load(f)['training'][0]['image'][2]  # To get image size, grab the T1 scan.\n",
    "else:\n",
    "    logger.info('ğŸ–¼ï¸ Using single scan...')\n",
    "    test_instance = inference_cfg['input']['single_scan']['t1']  # To get image size, grab the T1 scan.\n",
    "    json_data = {\n",
    "        'training': [\n",
    "            {\n",
    "                'fold': fold,\n",
    "                'image': [\n",
    "                    inference_cfg['input']['single_scan']['flair'],\n",
    "                    inference_cfg['input']['single_scan']['t1c'],\n",
    "                    inference_cfg['input']['single_scan']['t1'],\n",
    "                    inference_cfg['input']['single_scan']['t2']\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    if do_ground_truth:\n",
    "        json_data['training'][0]['label'] = inference_cfg['input']['single_scan']['ground_truth']\n",
    "\n",
    "    logger.info('ğŸ“ Creating temporary JSON file...')\n",
    "    with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as temp_file:  # Use delete=False to fix a permissions error on Windows.\n",
    "        json.dump(json_data, temp_file, indent=4)\n",
    "        temp_file.flush()\n",
    "        json_path = temp_file.name\n",
    "        _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
    "        temp_file.close()\n",
    "        os.unlink(temp_file.name)\n",
    "    logger.info('âœ… Temporary JSON file created and processed.')\n",
    "\n",
    "logger.info('ğŸ“ Loading image size...')\n",
    "resize_shape = list(nib.load(os.path.join(data_dir, test_instance)).shape)\n",
    "logger.debug(f'ğŸ“ Scan size: {resize_shape = }')\n",
    "\n",
    "logger.info('ğŸ”„ Setting up validation data loader...')\n",
    "val_loader = get_loader_val(\n",
    "    batch_size=1,\n",
    "    files=validation_files,\n",
    "    val_resize=resize_shape,\n",
    "    union=union,\n",
    "    workers=1,\n",
    "    cache_dir='',\n",
    "    dataset_type='Dataset',\n",
    "    add_label=do_ground_truth,\n",
    ")\n",
    "logger.info('âœ… Validation data loader setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_etOfVK4NJTo"
   },
   "source": [
    "# Inference\n",
    "This section takes the loaded model and runs inference on loaded scans, and saves the output to the location specified in your `config/config.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EH4JxkCUNJTo",
    "outputId": "c2c40d5c-e52d-44bc-e34d-01fe7d697f2b"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logger.info('ğŸ§® Setting up accuracy function...')\n",
    "    acc_func = DiceMetric(\n",
    "        include_background=True,\n",
    "        reduction=MetricReduction.MEAN_BATCH,\n",
    "    )\n",
    "    \n",
    "    i = 0\n",
    "    dice_scores = []\n",
    "    \n",
    "    logger.debug(f'ğŸ”¢ {i = }')\n",
    "    logger.debug(f'ğŸ“Š {dice_scores = }')\n",
    "\n",
    "    for val_data in val_loader:\n",
    "        logger.info('ğŸ“¥ Loading data...')\n",
    "        val_images = val_data['image']\n",
    "        logger.debug(f'ğŸ–¼ï¸ {val_images.shape = }')\n",
    "\n",
    "        logger.info('ğŸ“œ Loading affine matrix...')\n",
    "        if use_scan_list:\n",
    "            with open(json_path) as f:\n",
    "                d = json.load(f)\n",
    "            d['training'] = [entry for entry in d['training'] if entry['fold'] == fold]\n",
    "            affine_path = os.path.join(data_dir, d['training'][i]['image'][2])\n",
    "        else:\n",
    "            affine_path = os.path.join(data_dir, inference_cfg['input']['single_scan']['t1'])\n",
    "        affine = nib.load(affine_path).affine\n",
    "        logger.debug(f'ğŸ“ {affine.shape = }')\n",
    "\n",
    "        logger.info('ğŸ” Performing sliding window inference...')\n",
    "        val_outputs = sliding_window_inference(\n",
    "            val_images.to(device),\n",
    "            roi_size=roi_size,\n",
    "            sw_batch_size=4,\n",
    "            predictor=model,\n",
    "        )\n",
    "        logger.debug(f'ğŸ”¢ {val_outputs.shape = }')\n",
    "\n",
    "        logger.info('ğŸ”„ Applying post-processing...')\n",
    "        post_sigmoid = Activations(sigmoid=True)\n",
    "        post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "        val_outputs_convert = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs]\n",
    "        logger.debug(f'ğŸ”¢ {len(val_outputs_convert) = }')\n",
    "\n",
    "        if do_ground_truth:\n",
    "            logger.info('ğŸ“Š Calculating DICE scores...')\n",
    "            ground_truth = val_data['label']\n",
    "            acc_func.reset()\n",
    "            acc_func(y_pred=val_outputs_convert, y=ground_truth.to(device))\n",
    "            acc = acc_func.aggregate().cpu().numpy()\n",
    "            num_zeroes = [acc[0], acc[1], acc[2]].count(0.0)\n",
    "            mean = (acc[0] + acc[1] + acc[2]) / (3 - num_zeroes) if num_zeroes < 3 else 0  # Ignore cases with zero DICE\n",
    "            \n",
    "            logger.info(f'ğŸ“Š DICE (tc): {acc[0]}')\n",
    "            logger.info(f'ğŸ“Š DICE (wt): {acc[1]}')\n",
    "            logger.info(f'ğŸ“Š DICE (et): {acc[2]}')\n",
    "            logger.info(f'ğŸ“Š DICE (mean): {mean}')\n",
    "            dice_scores.append([acc[0], acc[1], acc[2], mean])\n",
    "\n",
    "        logger.info('ğŸ”„ Processing output...')\n",
    "        val_outputs = val_outputs.clone().cpu().numpy().squeeze()\n",
    "        logger.debug(f'ğŸ”¢ {val_outputs.shape = }')\n",
    "        segmentation_mask = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs]\n",
    "        print('segmentation_mask length',len(segmentation_mask))\n",
    "        segmentation_mask = segmentation_mask[:-1]  # Discard the union\n",
    "        print('segmentation_mask length',len(segmentation_mask))\n",
    "        print('segmentation_mask',segmentation_mask[0].shape)\n",
    "        print('segmentation_mask',segmentation_mask[1].shape)\n",
    "        print('segmentation_mask',segmentation_mask[2].shape)\n",
    "#         \n",
    "        segmentation_mask = torch.stack(segmentation_mask, dim=0)\n",
    "        segmentation_mask = np.sum(segmentation_mask, axis=0)\n",
    "#         segmentation_mask = torch.sum(torch.stack(segmentation_mask, dim=0), dim=0)\n",
    "        for k, v in {1: 5, 3: 4, 2: 1, 5: 2}.items():  # Correctly assign labels. Use 5 as a temporary for swapping.\n",
    "            segmentation_mask[segmentation_mask == k] = v\n",
    "\n",
    "        logger.info('ğŸ·ï¸ Identifying unique labels and their counts...')\n",
    "        unique_labels, label_counts = np.unique(segmentation_mask, return_counts=True)\n",
    "        logger.debug(f'ğŸ”¢ {unique_labels = }')\n",
    "        logger.debug(f'ğŸ”¢ {label_counts = }')\n",
    "        for label, count in zip(unique_labels, label_counts):\n",
    "            logger.info(f'ğŸ·ï¸ Label {int(label)}: {count} voxels')\n",
    "        total_voxels = np.prod(segmentation_mask.shape)\n",
    "        logger.info(f'ğŸ“Š Total voxels: {total_voxels}')\n",
    "        for label, count in zip(unique_labels, label_counts):\n",
    "            percentage = (count / total_voxels) * 100\n",
    "            logger.info(f'ğŸ“Š Label {int(label)}: {percentage:.2f}% of total volume')\n",
    "\n",
    "        logger.info('ğŸ§  Creating NIfTI image...')\n",
    "        nii = nib.Nifti1Image(segmentation_mask.astype(np.uint8), affine=affine)\n",
    "\n",
    "        if use_scan_list:  # If inferring on multiple scans, add the T1 file path to the name so they can be easily distinguished.\n",
    "            dirs = affine_path.replace('_T1_reg_SkullS_BiasC.nii.gz', '')\n",
    "#             dirs = affine_path.replace('_T1.nii.gz', '')\n",
    "            print(\"affine_path\", dirs)\n",
    "            dir2, fname = os.path.split(inference_cfg['output']['file_path'])\n",
    "#             print(\"dir2\", dir2)\n",
    "            print('split..',f'{dirs}_{fname}')\n",
    "            save_pth = os.path.join(dir2, f'{dirs}_{fname}')\n",
    "            \n",
    "        else:\n",
    "            save_pth = inference_cfg['output']['file_path']\n",
    "            \n",
    "        logger.info('ğŸ“ Creating output directory...')\n",
    "        output_directory = os.path.dirname(save_pth)\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        logger.info(f'ğŸ’¾ Saving as {save_pth}...')\n",
    "        nib.save(nii, save_pth)\n",
    "        i += 1\n",
    "\n",
    "if do_ground_truth:\n",
    "    logger.info('ğŸ“Š Calculating mean DICE scores...')\n",
    "    mask = np.ma.masked_equal(dice_scores, 0)  # Ignore cases with zero DICE\n",
    "    mean = mask.mean(axis=0).filled(np.nan)\n",
    "\n",
    "    logger.info('ğŸ“Š Mean DICE scores:')\n",
    "    logger.info(f'ğŸ“Š DICE (tc): {mean[0]}')\n",
    "    logger.info(f'ğŸ“Š DICE (wt): {mean[1]}')\n",
    "    logger.info(f'ğŸ“Š DICE (et): {mean[2]}')\n",
    "    logger.info(f'ğŸ“Š DICE (mean): {mean[3]}')\n",
    "\n",
    "logger.info('âœ… Inference complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, mask in enumerate(segmentation_mask):\n",
    "#     print(f\"Element {i} type: {type(mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mask in enumerate(segmentation_mask):\n",
    "    assert mask.shape == torch.Size([240, 240, 155]), f\"Shape mismatch at index {i}: {mask.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mask in enumerate(segmentation_mask):\n",
    "    assert mask.shape == torch.Size([240, 240, 155]), f\"Shape mismatch at index {i}: {mask.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask_sum = torch.sum(torch.stack(segmentation_mask, dim=0), dim=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
